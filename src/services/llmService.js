/**
 * ì§€ëŠ¥í˜• LLM ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤
 * OpenAI APIë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ ì œê³µ
 */

import openaiService from './openaiService.js'

class LLMService {
  constructor() {
    this.conversationHistory = []
    this.isProcessing = false
    this.useAgent = false
    this.agentPriority = true
  }

  // ë©”ì‹œì§€ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
  async processMessage(message, userId = 'user') {
    if (this.isProcessing) {
      throw new Error('ì´ì „ ìš”ì²­ì´ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.')
    }

    this.isProcessing = true
    let response = ''
    let source = 'fallback'

    try {
      // OpenAI API ì‚¬ìš© ì‹œë„
      if (openaiService.isReady()) {
        console.log('ğŸ¤– OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± ì¤‘...')
        const openaiResult = await openaiService.processMessage(message, userId)
        response = openaiResult.response
        source = 'openai'
        
        // OpenAI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        this.conversationHistory.push({
          role: 'user',
          content: message,
          timestamp: new Date().toISOString()
        })

        this.conversationHistory.push({
          role: 'assistant',
          content: response,
          timestamp: new Date().toISOString(),
          source: 'openai'
        })
        
      } else {
        console.log('ğŸ’¬ í´ë°± ì‘ë‹µ ìƒì„± ì¤‘...')
        response = this.generateFallbackResponse(message)
        source = 'fallback'
        
        // í´ë°± ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        this.conversationHistory.push({
          role: 'user',
          content: message,
          timestamp: new Date().toISOString()
        })

        this.conversationHistory.push({
          role: 'assistant',
          content: response,
          timestamp: new Date().toISOString(),
          source: 'fallback'
        })
      }

      return {
        success: true,
        response,
        conversationId: `conv_${Date.now()}`,
        source
      }

    } catch (error) {
      console.error('âŒ LLM ì„œë¹„ìŠ¤ ì˜¤ë¥˜:', error)
      
      // ì˜¤ë¥˜ ë°œìƒ ì‹œ í´ë°± ì‘ë‹µ
      response = `ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜: ${error.message}`
      
      this.conversationHistory.push({
        role: 'user',
        content: message,
        timestamp: new Date().toISOString()
      })

      this.conversationHistory.push({
        role: 'assistant',
        content: response,
        timestamp: new Date().toISOString(),
        source: 'error'
      })

      return {
        success: false,
        response,
        conversationId: `conv_${Date.now()}`,
        source: 'error',
        error: error.message
      }
      
    } finally {
      this.isProcessing = false
    }
  }

  // í´ë°± ì‘ë‹µ ìƒì„±
  generateFallbackResponse(message) {
    const responses = [
      "ì•ˆë…•í•˜ì„¸ìš”! Caesar AI Assistantì…ë‹ˆë‹¤. í˜„ì¬ ê°œë°œ ëª¨ë“œë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
      "ì§ˆë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë” ìì„¸í•œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
      "í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì´ë„¤ìš”! í˜„ì¬ëŠ” ë°ëª¨ ëª¨ë“œë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
      "ë„ì›€ì„ ë“œë¦¬ê³  ì‹¶ì§€ë§Œ, í˜„ì¬ëŠ” ì œí•œëœ ê¸°ëŠ¥ìœ¼ë¡œ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤.",
      "ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤! ì¶”í›„ ë” ë‚˜ì€ ì„œë¹„ìŠ¤ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    ]
    
    return responses[Math.floor(Math.random() * responses.length)]
  }

  // Agent ëª¨ë“œ í† ê¸€ (ë°ëª¨ìš©)
  async toggleAgentMode() {
    try {
      this.useAgent = !this.useAgent
      console.log(`${this.useAgent ? 'âœ… Agent ëª¨ë“œ í™œì„±í™”' : 'âŒ Agent ëª¨ë“œ ë¹„í™œì„±í™”'}`)
      return this.useAgent
    } catch (error) {
      console.error('Agent ëª¨ë“œ í† ê¸€ ì‹¤íŒ¨:', error)
      this.useAgent = false
      throw error
    }
  }

  // Agent ìƒíƒœ ì¡°íšŒ (ë°ëª¨ìš©)
  async getAgentStatus() {
    return {
      connected: this.useAgent,
      agentModeActive: this.useAgent,
      useAgent: this.useAgent
    }
  }

  // ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
  getConversationHistory() {
    return this.conversationHistory.map(msg => ({
      role: msg.role,
      content: msg.content,
      timestamp: msg.timestamp,
      source: msg.source || 'unknown'
    }))
  }

  // ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
  clearConversationHistory() {
    this.conversationHistory = []
    if (openaiService.isReady()) {
      openaiService.clearConversationHistory()
    }
    console.log('ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.')
  }

  // í˜„ì¬ ì²˜ë¦¬ ìƒíƒœ í™•ì¸
  isCurrentlyProcessing() {
    return this.isProcessing
  }

  // ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´
  getStatus() {
    return {
      useAgent: this.useAgent,
      agentPriority: this.agentPriority,
      openaiReady: openaiService.isReady(),
      agentReady: this.useAgent,
      isProcessing: this.isProcessing,
      conversationLength: this.conversationHistory.length,
      openaiSettings: openaiService.isReady() ? openaiService.getSettings() : null
    }
  }

  // Agent ìš°ì„ ìˆœìœ„ ì„¤ì •
  setAgentPriority(priority) {
    this.agentPriority = priority
    console.log(`ğŸ”„ Agent ìš°ì„ ìˆœìœ„: ${priority ? 'í™œì„±í™”' : 'ë¹„í™œì„±í™”'}`)
  }
}

export default new LLMService()